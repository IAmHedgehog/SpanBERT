{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'per:other_family', 'org:dissolved', 'per:stateorprovince_of_birth', 'per:country_of_birth', 'per:city_of_death', 'per:stateorprovinces_of_residence', 'org:political/religious_affiliation', 'per:cause_of_death', 'per:date_of_birth', 'per:countries_of_residence', 'org:founded_by', 'per:parents', 'per:cities_of_residence', 'per:siblings', 'per:origin', 'per:schools_attended', 'per:children', 'org:number_of_employees/members', 'per:religion', 'per:spouse', 'per:stateorprovince_of_death', 'per:date_of_death', 'per:country_of_death', 'per:charges', 'org:founded', 'org:shareholders'}\n",
      "3447\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "data = json.load(open('datasets/retacred/train.json'))\n",
    "\n",
    "re_stats = defaultdict(int)\n",
    "for sent in data:\n",
    "    re_stats[sent['relation']] += 1\n",
    "ignore_keys = set(['org:website', 'per:city_of_birth'])\n",
    "aug_keys = set()\n",
    "for key, value in sorted(re_stats.items(), key=lambda x: x[1]):\n",
    "    if value < 300 and key not in ignore_keys:\n",
    "        # print(f'---{key}: {value}')\n",
    "        aug_keys.add(key)\n",
    "print(aug_keys)\n",
    "aug_sents = [sent for sent in data if sent['relation'] in aug_keys]\n",
    "print(len(aug_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent):\n",
    "    tokens = list(sent['token'])\n",
    "    pairs = [(sent['subj_start'], sent['subj_end']+1), (sent['obj_start'], sent['obj_end']+1)]\n",
    "    pairs.sort()\n",
    "    poss = set()\n",
    "    for start, end in pairs:\n",
    "        if set(range(start, end)) & poss:\n",
    "            print('------> overlapping entities')\n",
    "            return None\n",
    "        poss.update(range(start, end))\n",
    "    for idx, (start, end) in enumerate(pairs):\n",
    "        tokens.insert(start+2*idx, '[[')\n",
    "        tokens.insert(end+2*idx+1, ']]')\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "encoded_sents = defaultdict(list)\n",
    "for sent in aug_sents:\n",
    "    # print(' '.join(sent['tokens']))\n",
    "    encoded_sent = encode_sent(sent)\n",
    "    if encoded_sent:\n",
    "        encoded_sents[sent['relation']].append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/Ying/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, prompt, device = \"cuda:4\"):\n",
    "        LLM_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "        self.prompt = prompt\n",
    "        self.device = device\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(LLM_path).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(LLM_path)\n",
    "\n",
    "    def chat(self, message):\n",
    "        content = f'{self.prompt}\\n{message}'\n",
    "        messages = [{\"role\": \"user\", \"content\": content}]\n",
    "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(self.device)\n",
    "        generated_ids = self.model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=self.tokenizer.eos_token_id)\n",
    "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
    "        return self.get_output(decoded[0])\n",
    "    \n",
    "    def get_output(self, response):\n",
    "        response = response.replace('<s>', '').replace('</s>', '')\n",
    "        if '[/INST]' not in response:\n",
    "            return response.replace('[INST]', '')\n",
    "        reponse = response.split('[/INST]')[1]\n",
    "        return reponse.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm = LLM(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(content):\n",
    "    pre_words = content.split()\n",
    "    while True:\n",
    "        changed = False\n",
    "        words = []\n",
    "        for word in pre_words:\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "            if word in ['[[', ']]']:\n",
    "                words.append(word)\n",
    "            elif '[[' in word:\n",
    "                idx = word.index('[[')\n",
    "                if len(word[:idx]):\n",
    "                    words.append(word[:idx])\n",
    "                words.append('[[')\n",
    "                if len(word[idx+2:]):\n",
    "                    words.append(word[idx+2:])\n",
    "                changed = True\n",
    "            elif ']]' in word:\n",
    "                idx = word.index(']]')\n",
    "                if len(word[:idx]):\n",
    "                    words.append(word[:idx])\n",
    "                words.append(']]')\n",
    "                if len(word[idx+2:]):\n",
    "                    words.append(word[idx+2:])\n",
    "                changed = True\n",
    "            elif not word[-1].isalpha():\n",
    "                words.append(word[:-1])\n",
    "                words.append(word[-1])\n",
    "            else:\n",
    "                words.append(word)\n",
    "        if not changed:\n",
    "            break\n",
    "        pre_words = words\n",
    "    return words\n",
    "\n",
    "\n",
    "def check_valid(words):\n",
    "    if '\\n' in words:\n",
    "        return False, 'multiple lines'\n",
    "    if words.count('[[') != 2:\n",
    "        return False, 'incorrect num of [[: %s' % words.count('[[')\n",
    "    if words.count(']]') != 2:\n",
    "        return False, 'incorrect num of ]]: %s' % words.count(']]')\n",
    "    head_start = words.index('[[')\n",
    "    head_end = words.index(']]')\n",
    "    tail_start = words.index('[[', head_start+1)\n",
    "    tail_end = words.index(']]', head_end+1)\n",
    "    if set(range(head_start, head_end+1)) & set(range(tail_start, tail_end+1)):\n",
    "        return False, 'overlap entities'\n",
    "    return True, 'Good content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> processing per:country_of_death\n",
      "--------> processing org:dissolved\n",
      "--------> processing per:country_of_birth\n",
      "--------> processing per:stateorprovince_of_birth\n",
      "--------> processing org:number_of_employees/members\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 21/54 [02:49<04:17,  7.82s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "template = \"\"\"\n",
    "You are an editor who is very good at reading sentence. Your task is rewrite a given sentence well keeping the original entities.\n",
    "\n",
    "In a sentence, each entity is nested in the sentence in the format of [[ entity ]].\n",
    "Rewrite the given sentence using each given entity exactly once and do not introduce other entities.\n",
    "Nest the original entities in the same format in the rewrited sentence.\n",
    "You change the content inside the entity.\n",
    "\n",
    "%s\n",
    "\"\"\"\n",
    "\n",
    "def rewrite_sent(sent, max_iter = 5):\n",
    "    message = template % sent\n",
    "    is_valid = False\n",
    "    cur_iter = 0\n",
    "    while not is_valid and cur_iter < max_iter:\n",
    "        response = llm.chat(message)\n",
    "        new_tokens = decode(response)\n",
    "        is_valid, error = check_valid(new_tokens)\n",
    "    if not is_valid:\n",
    "        print('=====not valid afater %s tries' % max_iter)\n",
    "        return None\n",
    "    return new_tokens\n",
    "\n",
    "for relation, cur_encoded_sents in sorted(encoded_sents.items(), key=lambda x: len(x[1])):\n",
    "    print('--------> processing', relation)\n",
    "    file_name = f'augs/{relation.replace(\"/\", \"--\")}.json'\n",
    "    if os.path.exists(file_name):\n",
    "        continue\n",
    "    rewrited_sents = []\n",
    "    for encoded_sent in tqdm(cur_encoded_sents):\n",
    "        rewrited_sent = rewrite_sent(encoded_sent)\n",
    "        if rewrited_sent:\n",
    "            # print('*' * 30)\n",
    "            # print(encoded_sent)\n",
    "            # print(rewrited_sent)\n",
    "            rewrited_sents.append(rewrited_sent)\n",
    "    with open(file_name, 'w') as af:\n",
    "        json.dump(rewrited_sents, af)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
