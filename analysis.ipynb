{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'org:political/religious_affiliation', 'per:cause_of_death', 'org:founded_by', 'org:shareholders', 'per:parents', 'org:dissolved', 'per:countries_of_residence', 'org:founded', 'per:siblings', 'per:city_of_death', 'per:origin', 'per:stateorprovinces_of_residence', 'per:country_of_death', 'per:spouse', 'per:date_of_death', 'per:children', 'per:other_family', 'per:stateorprovince_of_death', 'per:stateorprovince_of_birth', 'per:religion', 'per:schools_attended', 'per:charges', 'per:date_of_birth', 'org:number_of_employees/members', 'per:cities_of_residence', 'per:country_of_birth'}\n",
      "3447\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open('datasets/retacred/train.json'))\n",
    "\n",
    "re_stats = defaultdict(int)\n",
    "for sent in data:\n",
    "    re_stats[sent['relation']] += 1\n",
    "ignore_keys = set(['org:website', 'per:city_of_birth'])\n",
    "aug_keys = set()\n",
    "for key, value in sorted(re_stats.items(), key=lambda x: x[1]):\n",
    "    if value < 300 and key not in ignore_keys:\n",
    "        # print(f'---{key}: {value}')\n",
    "        aug_keys.add(key)\n",
    "print(aug_keys)\n",
    "aug_sents = [sent for sent in data if sent['relation'] in aug_keys]\n",
    "print(len(aug_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be026f1a7b794097869cf62a557d0b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aug\n",
    "\n",
    "\n",
    "importlib.reload(aug)\n",
    "\n",
    "template = \"\"\"\n",
    "You are an editor who is very good at paraphrasing sentences. Your task is rewrite a given sentence well keeping the original entities.\n",
    "\n",
    "In a sentence, two entities are nested in the sentence in the format of [[ entity ]].\n",
    "Rewrite the given sentence using each given entity exactly once and do not introduce other entities.\n",
    "Nest the original entities in the same format in the rewrited sentence.\n",
    "You can change the content inside the entity.\n",
    "\n",
    "%s\n",
    "\"\"\"\n",
    "\n",
    "encoded_sents = aug.get_encoded_sents(aug_sents)\n",
    "save_folder = 'augs'\n",
    "\n",
    "for relation, cur_encoded_sents in sorted(encoded_sents.items(), key=lambda x: len(x[1])):\n",
    "    print('--------> processing', relation, len(cur_encoded_sents))\n",
    "    file_name = f'{relation.replace(\"/\", \"--\")}.json'\n",
    "    save_path = os.path.join(save_folder, file_name)\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'skip {relation} due to {save_path} exists')\n",
    "        continue\n",
    "    rewrited_sents = []\n",
    "    for encoded_sent, head_ent_type, tail_ent_type, head_ent, tail_ent in tqdm(cur_encoded_sents):\n",
    "        message = template % encoded_sent\n",
    "        rewrited_sent = aug.rewrite_sent(llm, message)\n",
    "        if rewrited_sent:\n",
    "            # print('*' * 30)\n",
    "            # print(encoded_sent)\n",
    "            # print(rewrited_sent)\n",
    "            rewrited_sents.append((rewrited_sent, head_ent_type, tail_ent_type, head_ent, tail_ent))\n",
    "    print(f'--------> {len(rewrited_sents)} sentences generated')\n",
    "    print('*' * 50)\n",
    "    with open(save_path, 'w') as af:\n",
    "        json.dump(rewrited_sents, af)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix data in augmented sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "for key in ['train', 'dev']:\n",
    "    aug_files = glob(f'augs_{key}/*.json')\n",
    "    for aug_file in aug_files:\n",
    "        cur_data = json.load(open(aug_file))\n",
    "        new_data = []\n",
    "        for idx, (rewrited, head_ent_type, tail_ent_type, head_ent, tail_ent) in enumerate(cur_data):\n",
    "            sid = f'{os.path.basename(aug_file)[:-5]}_{idx}'\n",
    "            sent_info = {\n",
    "                'encoded_sent': [], 'head_ent_type': head_ent_type, 'tail_ent_type': tail_ent_type,\n",
    "                'rewrited_sent': rewrited, 'head_ent': head_ent, 'tail_ent': tail_ent, 'id': sid}\n",
    "            new_data.append(sent_info)\n",
    "        out_file = os.path.join(f'augs_{key}_2', os.path.basename(aug_file))\n",
    "        json.dump(new_data, open(out_file, 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many sentences augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: generated 22346 sentences for 23 relations\n",
      "dev set: generated 15152 sentences for 23 relations\n",
      "test set: generated 9471 sentences for 23 relations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "for key in ['train', 'dev', 'test']:\n",
    "    aug_files = glob(f'augs_{key}/*.json')\n",
    "    cnt = 0\n",
    "    relations = set()\n",
    "    for aug_file in aug_files:\n",
    "        cur_data = json.load(open(aug_file))\n",
    "        cnt += len(cur_data)\n",
    "        relation = os.path.basename(aug_file).split('_')[0]\n",
    "        relations.add(relation)\n",
    "    print(f'{key} set: generated {cnt} sentences for {len(relations)} relations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform augmented sentences into target data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: transformed 22322, total 80787\n",
      "dev set: transformed 15143, total 34727\n",
      "test set: transformed 9460, total 22878\n"
     ]
    }
   ],
   "source": [
    "import aug\n",
    "import json\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "\n",
    "importlib.reload(aug)\n",
    "\n",
    "for key in ['train', 'dev', 'test']:\n",
    "    folder = f'augs_{key}'\n",
    "    sents = aug.get_auged_sents(folder)\n",
    "\n",
    "    data = json.load(open(f'datasets/retacred/{key}.json'))\n",
    "    json.dump(sents+data, open(f'datasets/retacred_aug/{key}.json', 'w'))\n",
    "    print(f'{key} set: transformed {len(sents)}, total {len(sents+data)}')\n",
    "\n",
    "    # check label consistency\n",
    "    org_stats = defaultdict(int)\n",
    "    for sent in data:\n",
    "        org_stats[sent['relation']] += 1\n",
    "\n",
    "    stats = defaultdict(int)\n",
    "    for sent in sents+data:\n",
    "        stats[sent['relation']] += 1\n",
    "\n",
    "    assert len(set(stats.keys()) - set(org_stats.keys())) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "def get_pred_stats(train_file, test_file, pred_file, relations = None):\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_data = json.load(open(test_file))\n",
    "    gt_id2label = {sent['id']: sent['relation'] for sent in test_data if sent['relation'] != 'no_relation'}\n",
    "    \n",
    "    train_stats = defaultdict(int)\n",
    "    test_stats = defaultdict(int)\n",
    "    for sent in train_data:\n",
    "        if sent['relation'] == 'no_relation':\n",
    "            continue\n",
    "        train_stats[sent['relation']] += 1\n",
    "    train_stats['overall'] = sum(train_stats.values())\n",
    "    if relations is None:\n",
    "        relations = [relation for relation, _ in sorted(train_stats.items(), key=lambda x: x[1])]\n",
    "\n",
    "    test_class_stats = defaultdict(set)\n",
    "    test_pos_sids = set()\n",
    "    for sent in test_data:\n",
    "        if sent['relation'] == 'no_relation':\n",
    "            continue\n",
    "        test_stats[sent['relation']] += 1\n",
    "        sid = sent['id'].split('_')[0]\n",
    "        gt_id2label[sid] = sent['relation']\n",
    "        test_class_stats[sent['relation']].add(sid)\n",
    "        test_pos_sids.add((sid, sent['relation']))\n",
    "    test_stats['overall'] = sum(test_stats.values())\n",
    "\n",
    "    with open(pred_file) as pred_f:\n",
    "        lines = pred_f.readlines()\n",
    "    preds = defaultdict(list)\n",
    "    for line in lines:\n",
    "        sid, pred = line.strip().split()\n",
    "        # if pred == 'no_relation':\n",
    "        #     continue\n",
    "        sid = sid.split('_')[0]\n",
    "        preds[sid].append(pred)\n",
    "\n",
    "    all_preds = set()\n",
    "    class_preds = defaultdict(set)\n",
    "    for sid, s_preds in preds.items():\n",
    "        pred = max(set(s_preds), key = s_preds.count)\n",
    "        if pred == 'no_relation':\n",
    "            continue\n",
    "        all_preds.add((sid, pred))\n",
    "        class_preds[pred].add(sid)\n",
    "\n",
    "    TP = len(all_preds & test_pos_sids)\n",
    "    FP = len(all_preds - test_pos_sids)\n",
    "    FN = len(test_pos_sids - all_preds)\n",
    "    prec = TP / (TP + FP) if TP + FP else 0.0\n",
    "    recall = TP / (TP + FN) if TP + FN else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall else 0.0\n",
    "\n",
    "    print(TP, FP, FN, prec, recall, f1)\n",
    "\n",
    "    stats = {'overall': {'TP': TP, 'FP': FP, 'FN': FN, 'support': TP+FN, 'prec': prec, 'recall': recall, 'f1': f1}}\n",
    "    for relation in test_class_stats.keys():\n",
    "        class_TP = len(class_preds[relation] & test_class_stats[relation])\n",
    "        class_FP = len(class_preds[relation] - test_class_stats[relation])\n",
    "        class_FN = len(test_class_stats[relation] - class_preds[relation])\n",
    "        # print(relation, '=====>', TP, FP, FN)\n",
    "        class_prec = class_TP / (class_TP + class_FP) if (class_TP + class_FP) else 0\n",
    "        class_recall = class_TP / (class_TP + class_FN) if (class_TP + class_FN) else 0\n",
    "        class_f1 = 2 * class_prec * class_recall / (class_prec + class_recall) if (class_prec + class_recall) else 0\n",
    "        stats[relation] = {'TP': class_TP, 'FP': class_FP, 'FN': class_FN, 'support': class_TP+class_FN, 'prec': class_prec, 'recall': class_recall, 'f1': class_f1}\n",
    "    return train_stats, test_stats, stats, relations\n",
    "\n",
    "\n",
    "def write_stats2table(train_stats, test_stats, stats, relations, out_file):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Relation', 'TestSupport', 'TrainSupport', 'Prec', 'Recall', 'F1']\n",
    "    for relation in relations:\n",
    "        if relation == 'no_relation':\n",
    "            continue\n",
    "        row = [relation, test_stats[relation], train_stats[relation]]\n",
    "        if relation in stats:\n",
    "            row.extend([round(stats[relation]['prec'], 4), round(stats[relation]['recall'], 4), round(stats[relation]['f1'], 4)])\n",
    "        else:\n",
    "            row.extend([0.0, 0.0, 0.0])\n",
    "        table.add_row(row)\n",
    "\n",
    "    with open(out_file, 'w', newline='') as f_output:\n",
    "        f_output.write(table.get_csv_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> \n",
      "4614 907 1034 0.8357181669987321 0.8169263456090652 0.8262154176739189\n",
      "------> _aug_train\n",
      "4755 989 893 0.8278203342618384 0.8418909348441926 0.8347963483146067\n",
      "------> _aug_test\n",
      "4284 938 1364 0.8203753351206434 0.7584985835694051 0.7882244710211592\n",
      "------> _aug_train_aug_dev\n",
      "4737 978 911 0.8288713910761155 0.8387039660056658 0.8337586904866673\n",
      "------> _aug_train_aug_test\n",
      "4938 1026 710 0.8279678068410463 0.8742917847025495 0.850499483293145\n",
      "------> _aug_train_aug_dev_aug_test\n",
      "4920 1021 728 0.828143410200303 0.8711048158640227 0.849081025110018\n"
     ]
    }
   ],
   "source": [
    "org_train_file = 'datasets/retacred/train.json'\n",
    "aug_train_file = 'datasets/retacred_aug/train.json'\n",
    "org_test_file = 'datasets/retacred/test.json'\n",
    "aug_test_file = 'datasets/retacred_aug/test.json'\n",
    "relations = None\n",
    "\n",
    "for key in ['', '_aug_train', '_aug_test', '_aug_train_aug_dev', '_aug_train_aug_test', '_aug_train_aug_dev_aug_test']:\n",
    "    print('------>', key)\n",
    "    if 'aug_train' in key:\n",
    "        train_file = aug_train_file\n",
    "    else:\n",
    "        train_file = org_train_file\n",
    "    if 'aug_test' in key:\n",
    "        test_file = aug_test_file\n",
    "    else:\n",
    "        test_file = org_test_file\n",
    "    pred_file = f'tacred_dir{key}/predictions.txt'\n",
    "    out_file = f'tables/table{key}.csv'\n",
    "    train_stats, test_stats, stats, relations = get_pred_stats(train_file, test_file, pred_file, relations)\n",
    "    write_stats2table(train_stats, test_stats, stats, relations, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
